{
    "Mistral-7B-v0.3": {
        "n_block": 32,
        "n_layer": 2,
        "layer": ["self_attn", "mlp"],
        "layer_numel": {
            "self_attn": 41943040,
            "mlp": 176160768
        },
        "n_linear": 7,
        "linear": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj", "mlp.gate_proj", "mlp.up_proj", "mlp.down_proj"],
        "linear_shape": {
            "self_attn.q_proj": [4096, 4096],
            "self_attn.k_proj": [1024, 4096],
            "self_attn.v_proj": [1024, 4096],
            "self_attn.o_proj": [4096, 4096],
            "mlp.gate_proj": [14336, 4096],
            "mlp.up_proj": [14336, 4096],
            "mlp.down_proj": [4096, 14336]
        },
        "hierarchy": {"self_attn.q_proj": "self_attn", "self_attn.k_proj": "self_attn", "self_attn.v_proj": "self_attn", "self_attn.o_proj": "self_attn", "mlp.gate_proj": "mlp", "mlp.up_proj": "mlp", "mlp.down_proj": "mlp"},
        "model_numel": 6979321856,
        "model": "model", 
        "layers": "model.layers",
        "pre_layer": ["model.embed_tokens"],
        "post_layer": ["model.norm", "lm_head", "model.rotary_emb"]
    }
}