{
    "Llama-2-7b-hf": {
        "n_block": 32,
        "n_layer": 2,
        "layer": ["self_attn", "mlp"],
        "layer_numel": {
            "self_attn": 67108864,
            "mlp": 135266304
        },
        "n_linear": 7,
        "linear": ["self_attn.q_proj,self_attn.k_proj,self_attn.v_proj",  "self_attn.o_proj", "mlp.up_proj,mlp.gate_proj", "mlp.down_proj"],
        "linear_numel": {
            "self_attn.q_proj": 16777216,
            "self_attn.k_proj": 16777216,
            "self_attn.v_proj": 16777216,
            "self_attn.o_proj": 16777216,
            "mlp.gate_proj": 45088768,
            "mlp.up_proj": 45088768,
            "mlp.down_proj": 45088768
        },
        "hierarchy": {"self_attn.q_proj": "self_attn", "self_attn.k_proj": "self_attn", "self_attn.v_proj": "self_attn", "self_attn.o_proj": "self_attn", "mlp.gate_proj": "mlp", "mlp.up_proj": "mlp", "mlp.down_proj": "mlp"},
        "model_numel": 6476005376,
        "model": "model", 
        "layers": "model.layers",
        "pre_layer": ["model.embed_tokens"],
        "post_layer": ["model.norm", "model.rotary_emb", "lm_head"]
    },

    "Llama-2-13b-hf": {
        "n_block": 40,
        "n_layer": 2,
        "layer": ["self_attn", "mlp"],
        "layer_numel": {
            "self_attn": 104857600,
            "mlp": 212336640
        },
        "n_linear": 7,
        "linear": ["self_attn.q_proj,self_attn.k_proj,self_attn.v_proj",  "self_attn.o_proj", "mlp.up_proj,mlp.gate_proj", "mlp.down_proj"],
        "linear_numel": {
            "self_attn.q_proj": 26214400,
            "self_attn.k_proj": 26214400,
            "self_attn.v_proj": 26214400,
            "self_attn.o_proj": 26214400,
            "mlp.gate_proj": 70778880,
            "mlp.up_proj": 70778880,
            "mlp.down_proj": 70778880
        },
        "hierarchy": {"self_attn.q_proj": "self_attn", "self_attn.k_proj": "self_attn", "self_attn.v_proj": "self_attn", "self_attn.o_proj": "self_attn", "mlp.gate_proj": "mlp", "mlp.up_proj": "mlp", "mlp.down_proj": "mlp"},
        "model_numel": 12687769600,
        "model": "model", 
        "layers": "model.layers",
        "pre_layer": ["model.embed_tokens"],
        "post_layer": ["model.norm", "model.rotary_emb", "lm_head"]
    },

    "Llama-2-70b-hf": {
        "n_block": 80,
        "n_linear": 7,
        "linear": ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj", "mlp.down_proj", "mlp.up_proj", "mlp.gate_proj"]
    }
}